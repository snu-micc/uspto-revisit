{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import aiohttp\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from chemspipy import ChemSpider\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChemSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = [] # Change to your own API keys (list of API keys)\n",
    "\n",
    "# Global variables to track the current API key and request count\n",
    "current_key_index = 0\n",
    "request_count = 0\n",
    "REQUEST_LIMIT = 980  # Maximum requests per API key\n",
    "\n",
    "def get_current_api_key():\n",
    "    global current_key_index, request_count\n",
    "    \n",
    "    if request_count >= REQUEST_LIMIT:\n",
    "        # Switch to the next API key\n",
    "        current_key_index += 1\n",
    "        request_count = 0  # Reset the request count\n",
    "\n",
    "        if current_key_index >= len(API_KEYS):\n",
    "            current_key_index = 0  # Loop back to the first API key if we run out of keys\n",
    "    \n",
    "    return API_KEYS[current_key_index]\n",
    "\n",
    "def get_smiles_from_ChemSpider(compound_name):\n",
    "    global request_count\n",
    "    \n",
    "    api_key = get_current_api_key()\n",
    "    cs = ChemSpider(api_key)\n",
    "    \n",
    "    try:\n",
    "        results = cs.search(compound_name)\n",
    "        request_count += 1  # Increment the request count after a successful request\n",
    "\n",
    "        if results:\n",
    "            return results[0].smiles, 'ChemSpider'\n",
    "        else:\n",
    "            logging.error(f\"[ChemSpider] No results found for {compound_name}\")\n",
    "    except RequestException as re:\n",
    "        logging.error(f\"[ChemSpider] Network-related error for {compound_name}: {re}\")\n",
    "    except KeyError as ke:\n",
    "        logging.error(f\"[ChemSpider] KeyError when processing results for {compound_name}: {ke}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ChemSpider] Unexpected error occurred for {compound_name}: {e}\")\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def exponential_backoff(attempt, max_delay=60):\n",
    "    delay = min(max_delay, (2 ** attempt) + random.uniform(0, 1))\n",
    "    await asyncio.sleep(delay)\n",
    "\n",
    "async def fetch_smiles(session, url, semaphore, max_retries):\n",
    "    async with semaphore:  # Ensure the semaphore is respected\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 200:\n",
    "                        return await response.text()\n",
    "                    elif response.status == 429 or \"ServerBusy\" in await response.text():\n",
    "                        logging.warning(f\"[Busy] Server busy, retrying... (Attempt {attempt + 1}/{max_retries}) for URL: {url}\")\n",
    "                        await exponential_backoff(attempt)\n",
    "                    else:\n",
    "                        logging.error(f\"[Error] Failed with status code: {response.status} for URL: {url}\")\n",
    "                        response.raise_for_status()\n",
    "            except asyncio.TimeoutError:\n",
    "                logging.error(f\"[Timeout] Failed to fetch SMILES from {url} (Timeout)\")\n",
    "            except aiohttp.ClientError as e:\n",
    "                logging.error(f\"[ClientError] Failed to fetch SMILES from {url} [Error] {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    await exponential_backoff(attempt)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[Error] Failed to fetch SMILES from {url} [Error] {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    await exponential_backoff(attempt)\n",
    "    return None\n",
    "\n",
    "async def get_smiles_from_pubchem(session, compound_name, semaphore, max_retries):\n",
    "    pubchem_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{quote(compound_name)}/property/IsomericSMILES/JSON\"\n",
    "    try:\n",
    "        response_text = await fetch_smiles(session, pubchem_url, semaphore, max_retries)\n",
    "        if response_text:\n",
    "            data = json.loads(response_text)\n",
    "            if 'PropertyTable' in data and 'Properties' in data['PropertyTable'] and data['PropertyTable']['Properties']:\n",
    "                return data['PropertyTable']['Properties'][0]['IsomericSMILES'], 'PubChem'\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[PubChem] Failed to fetch SMILES for {compound_name} from URL: {pubchem_url} [Error] {e}\")\n",
    "    return None, None\n",
    "\n",
    "async def get_smiles_from_cir(session, compound_name, semaphore, max_retries):\n",
    "    cir_url = f\"http://cactus.nci.nih.gov/chemical/structure/{quote(compound_name)}/smiles\"\n",
    "    try:\n",
    "        return await fetch_smiles(session, cir_url, semaphore, max_retries), 'CIR'\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[CIR] Failed to fetch SMILES for {compound_name} from URL: {cir_url} [Error] {e}\")\n",
    "    return None, None\n",
    "\n",
    "async def get_smiles_from_opsin(session, compound_name, semaphore, max_retries):\n",
    "    opsin_url = f\"https://opsin.ch.cam.ac.uk/opsin/{quote(compound_name)}.smi\"\n",
    "    try:\n",
    "        return await fetch_smiles(session, opsin_url, semaphore, max_retries), 'OPSIN'\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[OPSIN] Failed to fetch SMILES for {compound_name} from URL: {opsin_url} [Error] {e}\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Caching system for already fetched SMILES\n",
    "smiles_cache = {}\n",
    "\n",
    "# Load existing cache\n",
    "if os.path.exists('smiles_cache.pkl'):\n",
    "    with open('smiles_cache.pkl', 'rb') as f:\n",
    "        smiles_cache = pickle.load(f)\n",
    "\n",
    "# Ensure directory exists\n",
    "def ensure_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# JSON validation and fixing functions\n",
    "def is_valid_json(json_string):\n",
    "    try:\n",
    "        json.loads(json_string)\n",
    "        return True\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Invalid JSON string: {json_string[:100]}... [Error] {e}\")\n",
    "        return False\n",
    "\n",
    "def fix_json_string(json_string):\n",
    "    try:\n",
    "        json_string = json_string.strip()\n",
    "        # Ensure JSON string starts with \"{\" and ends with \"}\"\n",
    "        if not json_string.startswith('{'):\n",
    "            json_string = '{' + json_string\n",
    "        if not json_string.endswith('}'):\n",
    "            json_string = json_string + '}'\n",
    "\n",
    "        # Count the number of opening and closing braces\n",
    "        open_braces = json_string.count('{')\n",
    "        close_braces = json_string.count('}')\n",
    "\n",
    "        # If there are more opening braces, add closing braces at the end\n",
    "        if open_braces > close_braces:\n",
    "            json_string += '}' * (open_braces - close_braces)\n",
    "            \n",
    "        # If there are more closing braces, add opening braces at the beginning\n",
    "        elif close_braces > open_braces:\n",
    "            json_string = '{' * (close_braces - open_braces) + json_string\n",
    "        return json_string\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fix JSON string: {json_string} [Error] {e}\")\n",
    "        return None\n",
    "\n",
    "def fix_name(compound_name):\n",
    "    remove_patterns = [\n",
    "        r\"\\d+\\s+normal\",\n",
    "        r\"\\d+(\\.\\d+)?\\s*N-\",\n",
    "        r\"\\d+(\\.\\d+)?\\s*N\",\n",
    "        r\"\\d+(\\.\\d+)?\\s*M\",\n",
    "        r\"\\d+%\",\n",
    "        r\"\\s*\\(\\s*\\)\",\n",
    "        r\"\\([^()]*\\)$\",\n",
    "        r\"Â·\",\n",
    "        r'\\([IVXLCDM]+\\)',\n",
    "        \"anhydrous\", \"concentrated\", \"catalyst\", \"-catalyst\", \"saturated\",\n",
    "        \"ice\", \"ice-\", \"dried\", \"aqueous\", \"solution\", \"normal\", \"solid\", \"complex\", \n",
    "        \"resin\", \"adduct\", \"corresponding\", \"atmosphere\", \"gas\", \"solvent\", \"crystal\", \n",
    "        \"crystals\", \"buffer\",\".conc\",\"fuming\",\"glacial\"\n",
    "    ]\n",
    "    pattern = f\"({'|'.join(remove_patterns)})\"\n",
    "    fixed_compound = re.sub(pattern, \"\", compound_name, flags=re.I)\n",
    "    return fixed_compound.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_smiles_from_ChemSpider_async(compound_name):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, get_smiles_from_ChemSpider, compound_name)\n",
    "\n",
    "async def get_smiles(session, compound_name, fix_name_bool, semaphore):\n",
    "    compound_name = compound_name.replace(\"â²\",\"'\")\n",
    "    if not isinstance(compound_name, str):\n",
    "        logging.warning(f\"Warning: compound_name is not a string: {compound_name} (type: {type(compound_name)})\")\n",
    "        compound_name = str(compound_name)\n",
    "\n",
    "    if fix_name_bool:\n",
    "        if compound_name in ['ice', 'DCM', 'DMA','ether','brine','Pd/C','DMSO','LiAlH4']:\n",
    "            return {'ice': 'O', 'DCM': 'C(Cl)Cl', 'DMA': 'CC(=O)N', 'ether': 'CCOCC', 'brine':'O.[Na+].[Cl-]','Pd/C':'Pd','DMSO':'CS(=O)C','LiAlH4':'[Li+].[AlH4-]'}.get(compound_name), 'Cache'\n",
    "        else:\n",
    "            compound_name = fix_name(compound_name)\n",
    "\n",
    "    if compound_name in smiles_cache:\n",
    "        return smiles_cache[compound_name], 'Cache'\n",
    "\n",
    "    tasks = [\n",
    "        get_smiles_from_opsin(session, compound_name, semaphore, max_retries=2),\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for result, source in results:\n",
    "        if result:\n",
    "            smiles_cache[compound_name] = result\n",
    "            logging.info(f\"Found SMILES for {compound_name} from {source}: {result}\")\n",
    "            return result, source\n",
    "        \n",
    "    logging.warning(f\"Failed to find SMILES for {compound_name} in all sources.\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "async def get_smiles_dict(response, session, fix_name_bool, semaphore):\n",
    "    smiles_dict = {}\n",
    "    problem_chemicals = []\n",
    "\n",
    "    async def process_chemicals(chemicals_dict, category, fix_name_bool):\n",
    "        tasks = {}\n",
    "        for code, compound_name in chemicals_dict.items():\n",
    "            tasks[code] = get_smiles(session, compound_name, fix_name_bool, semaphore)\n",
    "        \n",
    "        # print(tasks)\n",
    "        results = await asyncio.gather(*tasks.values(), return_exceptions=True)\n",
    "        for code, (smiles, source) in zip(tasks.keys(), results):\n",
    "            if not smiles:\n",
    "                fixed_name = fix_name(chemicals_dict[code])\n",
    "                fixed_smiles, fixed_source = await get_smiles(session, fixed_name, True, semaphore)\n",
    "                if not fixed_smiles:\n",
    "                    problem_chemicals.append(f\"{chemicals_dict[code]} ({category})\")\n",
    "                    smiles_dict[code] = f\"[{chemicals_dict[code]} (NoSmi)]\"\n",
    "                else:\n",
    "                    smiles_dict[code] = fixed_smiles\n",
    "                    logging.info(f\"Found SMILES for {chemicals_dict[code]} (Fixed Name) from {fixed_source}: {fixed_smiles}\")\n",
    "            else:\n",
    "                smiles_dict[code] = smiles\n",
    "                logging.info(f\"Found SMILES for {chemicals_dict[code]} from {source}: {smiles}\")\n",
    "\n",
    "    if 'Reactants, Solvents, Catalysts' in response:\n",
    "        await process_chemicals(response['Reactants, Solvents, Catalysts'], 'Reactant/Solvent/Catalyst', fix_name_bool)\n",
    "\n",
    "    if 'Product' in response:\n",
    "        await process_chemicals(response['Product'], 'Product', fix_name_bool)\n",
    "\n",
    "    if problem_chemicals:\n",
    "        logging.info(f\"Problem chemicals: {problem_chemicals}\")\n",
    "\n",
    "    return smiles_dict\n",
    "\n",
    "async def process_batch(json_responses, fix_name_bool, semaphore):\n",
    "    smiles_dicts = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for json_response in json_responses:\n",
    "            # Validate and fix JSON string\n",
    "            if not is_valid_json(json_response):\n",
    "                fixed_json = fix_json_string(json_response)\n",
    "                if fixed_json and is_valid_json(fixed_json):\n",
    "                    json_response = fixed_json\n",
    "                else:\n",
    "                    logging.error(f\"Skipping invalid JSON response: {json_response}\")\n",
    "                    continue  # Skip this invalid JSON string\n",
    "\n",
    "            # Process the valid JSON response\n",
    "            response_dict = json.loads(json_response)\n",
    "            tasks.append(get_smiles_dict(response_dict, session, fix_name_bool, semaphore))\n",
    "\n",
    "        smiles_dicts = await asyncio.gather(*tasks)\n",
    "    return smiles_dicts\n",
    "\n",
    "def save_smiles_dict(smiles_dict, filename):\n",
    "    with open(filename, 'w', encoding='utf-8-sig') as f:\n",
    "        json.dump(smiles_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def make_smiles_dict(df, batch_size=100, output_dir='smiles_batches'):\n",
    "    # Ensure the output directory exists\n",
    "    ensure_directory(output_dir)\n",
    "\n",
    "    smiles_dict = []\n",
    "    semaphore = asyncio.Semaphore(60)\n",
    "    \n",
    "    # Process batches\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing GPT Responses in Batches\"):\n",
    "        batch = df[i:i + batch_size]\n",
    "        batch_number = i // batch_size + 1\n",
    "        try:\n",
    "            temp_smiles_dicts = asyncio.run(process_batch(batch, fix_name_bool=False, semaphore=semaphore))\n",
    "            smiles_dict.extend(temp_smiles_dicts)\n",
    "            logging.info(f\"Completed batch {batch_number}/{(len(df) + batch_size - 1) // batch_size}\")\n",
    "            \n",
    "            # Save intermediate results\n",
    "            save_smiles_dict(smiles_dict, os.path.join(output_dir, f'smiles_dict_batch_{batch_number}.json'))\n",
    "\n",
    "            # Save the cache periodically\n",
    "            with open(os.path.join(output_dir, 'smiles_cache.pkl'), 'wb') as f:\n",
    "                pickle.dump(smiles_cache, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            smiles_dict.extend([\"Error\"] * len(batch))\n",
    "            logging.error(f\"Error in batch {batch_number}: {e}\")\n",
    "    \n",
    "    save_smiles_dict(smiles_dict, os.path.join(output_dir, 'smiles_dict_final_ver1.json'))\n",
    "    return smiles_dict\n",
    "\n",
    "# Function to monitor log in real-time\n",
    "def monitor_log(file_path, stop_event, lines_per_batch=1):\n",
    "    with open(file_path, 'r') as f:\n",
    "        f.seek(0, 2)  # Go to the end of the file\n",
    "        batch_counter = 0\n",
    "        line_counter = 0\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                if \"Starting batch\" in line:  # Reset line counter for each new batch\n",
    "                    line_counter = 0\n",
    "                    batch_counter += 1\n",
    "                    print(f\"--- Batch {batch_counter} ---\")\n",
    "                \n",
    "                if line_counter < lines_per_batch:\n",
    "                    print(line, end='')\n",
    "                    line_counter += 1\n",
    "                \n",
    "                if \"Processing completed\" in line:\n",
    "                    print(\"All batches processed. Stopping log monitoring.\")\n",
    "                    stop_event.set() \n",
    "            else:\n",
    "                time.sleep(0.1)  # Wait briefly before checking again\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SMILES from OPSIN first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GPT Responses in Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 18:39:55,800 - ERROR - [Error] Failed with status code: 404 for URL: https://opsin.ch.cam.ac.uk/opsin/MeOH.smi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GPT Responses in Batches: 100%|ââââââââââ| 5/5 [02:06<00:00, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed\n",
      "All batches processed. Stopping log monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_processing():\n",
    "    make_smiles_dict(df['GPT_finetuned_five'], output_dir=output_directory)\n",
    "    logging.info(\"Processing completed\")\n",
    "    print(\"Processing completed\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='smiles_fetch.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# CSV íì¼ì ì½ê³ , make_smiles_dict í¨ì í¸ì¶\n",
    "df = pd.read_csv(\"GPT_response.csv\")\n",
    "\n",
    "# Specify the output directory\n",
    "output_directory = \"smiles_batches\"\n",
    "\n",
    "# Create a stop event for the log monitoring\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start the processing thread\n",
    "processing_thread = threading.Thread(target=run_processing)\n",
    "\n",
    "# Start the processing thread\n",
    "processing_thread.start()\n",
    "\n",
    "# Monitor the log in real-time\n",
    "try:\n",
    "    monitor_log('smiles_fetch.log', stop_event)\n",
    "finally:\n",
    "    # Once processing is done, signal the log monitoring to stop\n",
    "    stop_event.set()\n",
    "    processing_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SMILES from other APIs: PubChem, CIR, ChemSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "def check_server_status(server_name, test_url):\n",
    "    try:\n",
    "        response = requests.get(test_url)\n",
    "        if response.status_code == 200:\n",
    "            return f\"{server_name} server is up and running.\", True\n",
    "        elif response.status_code == 429:\n",
    "            return f\"{server_name} server is overloaded (Too Many Requests).\", False\n",
    "        elif response.status_code == 503:\n",
    "            return f\"{server_name} server is currently unavailable (Service Unavailable).\", False\n",
    "        else:\n",
    "            return f\"{server_name} server returned status code {response.status_code}.\", False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Failed to reach {server_name} server: {e}\", False\n",
    "    \n",
    "def calculate_no_smi_percentage(smiles_dict_list):\n",
    "    total_entries = 0\n",
    "    no_smi_entries = 0\n",
    "\n",
    "    for smiles_dict in smiles_dict_list:\n",
    "        total_entries += len(smiles_dict)\n",
    "        no_smi_entries += sum(1 for value in smiles_dict.values() if \"(NoSmi)\" in value)\n",
    "    \n",
    "    no_smi_percentage = (no_smi_entries / total_entries) * 100 if total_entries > 0 else 0\n",
    "    logging.info(f\"Found {no_smi_entries} entries with NoSmi [{no_smi_percentage:.2f}%].\")\n",
    "    return no_smi_percentage\n",
    "\n",
    "async def process_no_smi_entry(code, compound_name, session, semaphore, smiles_dict,idx):\n",
    "    tasks = [\n",
    "        get_smiles_from_cir(session, compound_name, semaphore, max_retries=5),\n",
    "        get_smiles_from_ChemSpider_async(compound_name),\n",
    "        get_smiles_from_pubchem(session, compound_name, semaphore, max_retries=5)\n",
    "    ]\n",
    "    \n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        try:\n",
    "            smiles, source = await task\n",
    "            if smiles:\n",
    "                logging.info(f\"[{idx}] Found SMILES for {compound_name} from {source}: {smiles}\")\n",
    "                smiles_dict[code] = smiles  # Update directly in the original dictionary\n",
    "                return\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[{idx}] Error processing {compound_name}: {e}\")\n",
    "\n",
    "    logging.info(f\"[{idx}] No SMILES found for {compound_name}\")\n",
    "    smiles_dict[code] = f\"[{compound_name} (NoSmi)]\"  # Mark as NoSmi if not found\n",
    "\n",
    "\n",
    "async def process_batch(smiles_dict_list, session, semaphore):\n",
    "    tasks = []\n",
    "    for idx, smiles_dict in enumerate(smiles_dict_list):\n",
    "        no_smi_entries = {key: value for key, value in smiles_dict.items() if \"(NoSmi)\" in value}\n",
    "        for code, compound_name_with_no_smi in no_smi_entries.items():\n",
    "            compound_name = compound_name_with_no_smi.replace(\"(NoSmi)\", \"\").strip('[] ')\n",
    "            tasks.append(process_no_smi_entry(code, compound_name, session, semaphore, smiles_dict, idx))\n",
    "    \n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def reprocess_no_smi(smiles_dict_file, output_file, session, semaphore, batch_size):\n",
    "    with open(smiles_dict_file, 'r', encoding='utf-8-sig') as f:\n",
    "        smiles_dict_list = json.load(f)\n",
    "\n",
    "    calculate_no_smi_percentage(smiles_dict_list)\n",
    "\n",
    "    total_batches = (len(smiles_dict_list) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "    with tqdm(total=total_batches, desc=\"Processing Batches\", unit=\"batch\") as pbar:\n",
    "        for i in range(0, len(smiles_dict_list), batch_size):\n",
    "            batch = smiles_dict_list[i:i + batch_size]\n",
    "            await process_batch(batch, session, semaphore)\n",
    "\n",
    "            # Save progress after each batch\n",
    "            with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "                json.dump(smiles_dict_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            logging.info(f\"Batch {i//batch_size + 1}/{total_batches} processed.\")\n",
    "            pbar.update(1)  # Update the progress bar after each batch\n",
    "        logging.info(\"Processing completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_reprocess():\n",
    "    semaphore = asyncio.Semaphore(40)  # Limit concurrent tasks\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        stop_event = threading.Event()\n",
    "        processing_thread = threading.Thread(target=monitor_log, args=('smiles_fetch.log', stop_event))\n",
    "        processing_thread.start()\n",
    "\n",
    "        await reprocess_no_smi('./smiles_batches/smiles_dict_final_ver1.json', './smiles_batches/smiles_dict_final_updated_ver2.json', session, semaphore, batch_size=100)\n",
    "        stop_event.set()\n",
    "        processing_thread.join()\n",
    "\n",
    "# Run the async main function\n",
    "asyncio.run(main_reprocess())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uspto_revisit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
